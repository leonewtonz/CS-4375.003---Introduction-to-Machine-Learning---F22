---
output:
  html_document: default
  pdf_document: default
---
# Similarity

#### CS 4375 - Intro to Machine Learning
#### Dr. Karen Mazidi
#### Author: 

  *   Leo Nguyen - ldn190002
  *   Simon Kim - sxk190106
  *   Abed Ahmed - asa190005
  *   Dylan Kapustka - dlk190000


## Physicochemical Properties of Protein Tertiary Structure Data Set

**Citation:** 

ABV - Indian Institute of Information Technology & Management, Gwalior, MP, India.
UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.

**Link for Data Set**
https://archive.ics.uci.edu/ml/datasets/Physicochemical+Properties+of+Protein+Tertiary+Structure

**Attribute Information:**

  - RMSD-Size of the residue.
  - F1 - Total surface area.
  - F2 - Non polar exposed area.
  - F3 - Fractional area of exposed non polar residue.
  - F4 - Fractional area of exposed non polar part of residue.
  - F5 - Molecular mass weighted exposed area.
  - F6 - Average deviation from standard exposed area of residue.
  - F7 - Euclidean distance.
  - F8 - Secondary structure penalty.
  - F9 - Spacial Distribution constraints (N,K Value).

### Load the data
```{r}
df <- read.csv("data/CASP.csv", header=TRUE)
str(df)
```
### Data Cleaning

Remove the column RMSD

```{r}
df = subset(df, select = -c(RMSD))
str(df)
```


### Divide into 80/20 train/test

```{r}
set.seed(1234)
i <- sample(1:nrow(df), 0.8*nrow(df), replace=FALSE)
train <- df[i,]
test <- df[-i,]
```

### Data Exploration

1. Look at the first 10 rows in training data

```{r}
head(train, n=10)
```

2. View the summary of entire training data set

```{r}
summary(train)
```

3. Count number of NA value by column

```{r}
colSums(is.na(train)) 
```

4. Check the correlation of all column in training data

  - Based on the number we can see that F1 is highly correlation with F2, F4, F5, F6, F9
  
```{r}
cor(train)
```

5. Quick visualization on correlation 

  - These plots confirm the highly correlation of F1 and other attributes mentioned  above
  
```{r}
pairs(train)
```



6. Summary() for F1 and F5 in training data

**Summary for F1 - Total surface area.**

```{r}
summary(train$F1)
```

**Summary for F5 - Molecular mass weighted exposed area.**

```{r}
summary(train$F5)
```


### Data Visualization using informative graph

1. Plot show the relationship between F1 - Total surface area with:

  -   F5 - Molecular mass weighted exposed area. (Most correlation with F1)
  -   F2 - Non polar exposed area. (Some correlation with F1)
  -   F3 - Fractional area of exposed non polar residue. (Leasst correclation with F1)
  
```{r}
par(mfrow=c(1,3))
plot(train$F5,train$F1 ,xlab = "F5 - Molecular mass weighted exposed area.", ylab = "F1 - Total surface area." )
plot(train$F2,train$F1 ,xlab = "F2 - Non polar exposed area.", ylab = "F1 - Total surface area." )
plot(train$F3,train$F1 ,xlab = "F3 - Fractional area of exposed non polar residue.", ylab = "F1 - Total surface area." )
```
```{r}
hist(train$F1, xlab = "F1 - Total surface area", main = "Total surface are of Protein Tertiary Structure")
```

## Comparing 3 models using correlation metric and mse

### Linear Regression Model using all other column as predictors

**Build and output the summary**

```{r}
lm <- lm(F1~., data=train)
summary(lm)
```

**Calculate correlation, mse and rmse for Linear Regression lm**

```{r}
pred_lm <- predict(lm, newdata=test)
cor_lm <- cor(pred_lm, test$F1)
mse_lm <- mean((pred_lm-test$F1)^2) 
rmse_lm <- sqrt(mse_lm)
print(paste('correlation:', cor_lm))
print(paste('mse:', mse_lm))
print(paste('rmse:', rmse_lm))
```


### kNN Regression

**Build kNN Regression Model**

Load the require library for kNN Regression**

```{r}
library(caret)
```

**First we need to find the best k value**

```{r}
cor_k <- rep(0, 20)
mse_k <- rep(0, 20)
i <- 1
for (k in seq(1, 39, 2)){
  fit_k <- knnreg(train[,2:9],train[,1],k=k)
  pred_k <- predict(fit_k, test[,2:9])
  cor_k[i] <- cor(pred_k, test$F1)
  mse_k[i] <- mean((pred_k - test$F1)^2)
  print(paste("k=", k, cor_k[i], mse_k[i]))
  i <- i + 1
}
```

Some visualization on the output

```{r}
plot(1:20, cor_k, lwd=2, col='red', ylab="", yaxt='n')
par(new=TRUE)
plot(1:20, mse_k, lwd=2, col='blue', labels=FALSE, ylab="", yaxt='n')
```

The best k value is 3

```{r}
which.min(mse_k)
which.max(cor_k)
```


**We fit the model using all predicor with k=3**

```{r}
fit <- knnreg(train[,2:9],train[,1],k=3)
```

**Calculate correlation, mse and rmse for kNN Regression**

```{r}
pred_knn <- predict(fit, test[,2:9])
cor_knn <- cor(pred_knn, test$F1)
mse_knn <- mean((pred_knn - test$F1)^2)
rmse_knn <- sqrt(mse_knn)
print(paste('correlation:', cor_knn))
print(paste('mse:', mse_knn))
print(paste('rmse:', rmse_knn))

```


### Decision Tree Regression

**Build Decision Regression Model**

Load require library for decision tree model

```{r}
library(tree)
```

Perform the decision tree regression model with all predictors. As we can see the model is only actually use F5 as the predictor.
It is because tree use some indication such as entropy, information gain and Gini index to select a good information variables.
This confirm the information we have when we exploring the data above (Check the graph between F1 vs F2, F5, F3)

```{r}
tree <- tree(F1~., data=train)
summary(tree)
```


**Calculate correlation, mse and rmse for Decision Tree Regression**


```{r}
pred_tree <- predict(tree, newdata=test)
cor_tree <- cor(pred_tree, test$F1)
mse_tree <- mean((pred_tree - test$F1)^2)
rmse_tree <- sqrt(mse_tree)
print(paste('correlation:', cor_tree))
print(paste('mse:', mse_tree))
print(paste('rmse:', rmse_tree))
```

Plotting the tree

```{r}
plot(tree)
text(tree, cex=0.5, pretty=0)
```


### Comparing results and analysis on why the results were most likely achived


| Model                    | Correlation       | rmse             |
|--------------------------|-------------------|------------------|
| Linear Regression        | 0.998812915661555 | 198.129236023442 |
| kNN Regression           | 0.998008972762981 | 256.639383652896 |
| Decision Tree Regression | 0.981994447324154 | 769.473275456637 |


As you can see in the summary table above, the Linear Regression give the best result. It has the highest correlation and lowest rmse among 3 model. There is no surprise, as when we explore the data, we can see that, the data set is highly linear, specially with some attributes such as F2, F4, F5, F6, F9. The next ranking model is kNN Regression. Even though, kNN Regression gave a good correlation and rmse, it is not very useful for us, as it is a non-parametric model, it did not provide any coefficient. The last ranking is Decision Tree Regression, it is the because even though we instruction the model to use all the attributes as predictor, the model only pick F5 as the best information attribute. That's why we have a bigger drop on correlation and rmse compare to kNN Regression

