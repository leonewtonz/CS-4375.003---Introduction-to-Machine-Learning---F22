# Kernel and Ensemble Methods
## Ensemble Techniques

#### CS 4375 - Intro to Machine Learning
#### Dr. Karen Mazidi
#### Author: 

  *   Leo Nguyen - ldn190002
  *   Cory Pekkala - cdp190005


## Skin Segmentation Data Set

**Data Set Information**

-   The skin dataset is collected by randomly sampling B,G,R values from face images of various age groups (young, middle, and old), race groups (white, black, and asian), and genders obtained from FERET database and PAL database. Total learning sample size is 245057; out of which 50859 is the skin samples and 194198 is non-skin samples. Color FERET Image Database: [Web Link], PAL Face Database from Productive Aging Laboratory, The University of Texas at Dallas: [Web Link].

**Citation:** 

-   Rajen Bhatt, Abhinav Dhall, 'Skin Segmentation Dataset', UCI Machine Learning Repository
-   https://archive.ics.uci.edu/ml/datasets/Skin+Segmentation

**Attribute Information:**

-   This dataset is of the dimension 245057 * 4 where first three columns are B,G,R (x1,x2, and x3 features) values and fourth column is of the class labels (decision variable y).


### Load the data
```{r}
df_origin <- read.csv("data/Skin Segmentation.csv", header=TRUE)
str(df_origin)
dim(df_origin)
```

### Data Cleaning

**Create a subsample by produce a subset randomly from a data frame**

-   Just randomly pick up 10000 observations to create a subset

```{r}
set.seed(1234)
df <- df_origin[sample(1:nrow(df_origin), 10000, replace = FALSE),]
str(df)
dim(df)
```

-   Convert column Skin to factor, and label them as Skin and NonSkin

```{r}
df$Skin <- factor(df$Skin)
levels(df$Skin) <- c("Skin", "NonSkin")
str(df)
```

### Divide into train, test (80/20)

```{r}
set.seed(1234)
i <- sample(1:nrow(df), 0.8*nrow(df), replace=FALSE)
train <- df[i,]
test <- df[-i,]
```

### Data Exploration

1. View the summary of entire training data set

```{r}
summary(train)
```
2. Skin factor encoding

```{r}
contrasts(train$Skin)
```


### Data Visualization using informative graph

1. Using barplots to visualize the 2 factor level of Skin Sample

```{r}
counts <- table(train$Skin)
barplot(counts, xlab="Skin Sample", ylab="Frequency", col=c("wheat", "seagreen"))

```

2. Visualize the relationship between Skin and B, G, R

```{r}
par(mfrow=c(1,3))
plot(train$Skin, train$B, data = train, main = "Blue", col="blue")
plot(train$Skin, train$G, data = train, main = "Green", col="green")
plot(train$Skin, train$R, data = train, main = "Red", col="red")
```


### Decision Tree

**Install mltools: Machine Learning Tool**

-   This package will be used to calculate the MATTHEWâ€™S CORRELATION COEFFICIENT (mcc) which an important to indicate the differences in class distribution

-   You can install the package by typing install.packages("mltools") on the console

```{r}
library(mltools)
```


**Building Decision Tree as baseline model**

```{r}
library(tree)
tree <- tree(Skin~., data=train)
tree
summary(tree)
```

**Plotting Decision Tree**

-   Plotting Decision Tree to get a visualization on the model


```{r}
plot(tree)
text(tree, cex=0.5, pretty=0)
```


**Evaluate Decision Tree**

```{r}
startTime <- Sys.time()
pred_dt <- predict(tree, newdata = test, type="class")
endTime <- Sys.time()

table(pred_dt, test$Skin)
acc_dt <- mean(pred_dt==test$Skin)
mcc_dt <- mcc(pred_dt, test$Skin)
runtime_dt <- endTime - startTime

print(paste('Accuracy:', acc_dt))
print(paste('mcc:', mcc_dt))
print(paste('Runtime in seconds:', runtime_dt))
```

-   The Decision Tree (DT) get a very high accuracy = 0.984. This indicate DT is very good model. Moreover, the mcc=0.95545 is also very high which further more confirm that. It is because, the accuracy by itself is not work well when the data is in balance which is the case of our dataset (mentioned in the dataset description above:  50859 is the skin samples and 194198 is non-skin samples). In this case, the accuracy alone will provide an overoptimistic estimation based on the majority class. By combining the mcc into the evaluation we can double check and verify this overoptimistic estimation. The mcc=0.95545 very close to 1 which indicate the total agreement between test data and predict data

### Random Forest

**Building Random Forest Model**

```{r}
library(randomForest)
set.seed(1234)
rf <- randomForest(Skin~., data=train, importance=TRUE)
rf
summary(rf)
```


**Evaluate Random Forest**

```{r}
startTime <- Sys.time()
pred_rf <- predict(rf, newdata = test, type="response")
endTime <- Sys.time()

table(pred_rf, test$Skin)
acc_rf <- mean(pred_rf==test$Skin)
mcc_rf <- mcc(pred_rf, test$Skin)
runtime_rf <- endTime - startTime

print(paste('Accuracy:', acc_rf))
print(paste('mcc:', mcc_rf))
print(paste('Runtime in seconds:', runtime_rf))
```

-   As an ensemble model,the Random Forest (RF) improve the accuracy and mcc further more by using parallel ensemble method-bagging with DT as individual model. Both accuracy=0.999 and mcc=0.99711 is very high, they are almost 100%. When looking at the RT model summary, we can see that by using 500 different DT, we can get different value on accuracy and mcc. Then when combine all this prediction together, we can achieve a final prediction which much higher accuracy and mcc compare to base model.

### XGBoost

**Building XGBoost**


```{r}
library(xgboost)
train_label <- ifelse(as.integer(train$Skin)==2, 1, 0)
train_matrix <- data.matrix(train[, -4])
xg <- xgboost(data=train_matrix, label=train_label,
                 nrounds=100, objective='binary:logistic')
xg
```


**Evaluate XGBoost**


```{r}
startTime <- Sys.time()
test_label <- ifelse(as.integer(test$Skin)==2, 1, 0)
test_matrix <- data.matrix(test[, -4])
probs <- predict(xg, test_matrix)
pred <- ifelse(probs>0.5, 1, 0)
endTime <- Sys.time()

acc_xg <- mean(pred==test_label)
mcc_xg <- mcc(pred, test_label)
runtime_xg <- endTime - startTime

print(paste("accuracy=", acc_xg))
print(paste("mcc=", mcc_xg))
print(paste('Runtime in seconds:', runtime_xg))

```
-   The XGBoost is also an ensemble method, that is why we can see improvement on accuracy=0.9985 and mcc=0.99567 compare to DT. However, it is slightly lower than the Random Forest. This could be because in this specific XGBoost model, we only use 100 iterations which is much less than 500 trees in Random Forest. It mean we still have room to improve the accuracy and mcc by increasing the number of iteration when building the model. Due to it unique algorithm, the XGBoost also the fast model in all 4 example models: Decision Tree, Random Forest, AdaBoost and XGBoost. It has the lowest runtime among 4 models.


### AdaBoost

**Building Adaboost**

```{r}
library(adabag)
adab1 <- boosting(Skin~., data=train, boos=TRUE, mfinal=20, coeflearn='Breiman')
summary(adab1)
```

**Evaluate Adaboost**

```{r}
startTime <- Sys.time()
pred_adabag <- predict(adab1, newdata = test, type="response")
endTime <- Sys.time()

table(pred_adabag$class, test$Skin)
acc_adabag <- mean(pred_adabag$class==test$Skin)
mcc_adabag <- mcc(factor(pred_adabag$class), test$Skin)
runtime_adabag <- endTime - startTime

print(paste('Accuracy:', acc_adabag))
print(paste('mcc:', mcc_adabag))
print(paste('Runtime in seconds:', runtime_adabag))
```
-   The AdaBoost also achieve a very high accuracy=0.999 and mcc=0.99711 which is no surprise there. It is because AdaBoost is an ensemble model. However, it use different methods compare to the Random Forest(parallel), AdaBoost is sequential. There is some trade off to achieve this high accuracy and mcc value, the AdaBoost the is the lowest model among 4. It is because of its natural, a sequential method.

### Conclusion

-   The Decision Tree can provide a good accuracy and good mcc value. By using other ensemble model we can achieve an even higher accuracy and mcc value. Then using the DT as a base model, we can see that Random Forest, XGBoost and AdaBoost provide a much higher accuracy and mcc value. However, there is a trade off on the runtime. If the ensemble model is parallel (like in Random Forest), we can get both good accuracy and good runtime. If the model is sequenstial (like AdaBoost), we can get a good accuracy but the runtime increase compare to base model (Decision Tree). Another case  is XGBoost model. It is the fastest model with lowest runtime compare from both base model and ensemble model. However, there some trade off as the accuracy and mcc slightly reduce compare with other ensemble model, but it is still higher than base model.

